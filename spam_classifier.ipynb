{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import email\n",
    "import email.policy\n",
    "import re\n",
    "from html import unescape\n",
    "import nltk\n",
    "import urlextract\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the data\n",
    "\n",
    "  - spam: 500 spam messages, all received from non-spam-trap sources.\n",
    "\n",
    "  - easy_ham: 2500 non-spam messages.  These are typically quite easy to\n",
    "    differentiate from spam, since they frequently do not contain any spammish\n",
    "    signatures (like HTML etc).\n",
    "    \n",
    "  - hard_ham: 250 non-spam messages which are closer in many respects to\n",
    "    typical spam: use of HTML, unusual HTML markup, coloured text,\n",
    "    \"spammish-sounding\" phrases etc.\n",
    "  \n",
    "Source: https://spamassassin.apache.org/old/publiccorpus/readme.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "DATASETS_PATH = os.path.join(\"Datasets\")\n",
    "FILES = [\"20030228_easy_ham.tar.bz2\",\n",
    "         \"20030228_hard_ham.tar.bz2\",\n",
    "         \"20030228_spam.tar.bz2\",]\n",
    "DOWNLOAD_URLS = [DOWNLOAD_ROOT + name for name in FILES]\n",
    "\n",
    "def fetch_data():\n",
    "    if not os.path.isdir(DATASETS_PATH):\n",
    "        os.makedirs(DATASETS_PATH)\n",
    "        \n",
    "    for filename, url in list(zip(FILES, DOWNLOAD_URLS)):\n",
    "        path = os.path.join(DATASETS_PATH, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        current_tar_file = tarfile.open(path)\n",
    "        current_tar_file.extractall(path=DATASETS_PATH)\n",
    "        current_tar_file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam emails: 500 \n",
      "Non-spam emails: 2750\n"
     ]
    }
   ],
   "source": [
    "spam_path = os.path.join(DATASETS_PATH, 'spam')\n",
    "ham_path = os.path.join(DATASETS_PATH, 'easy_ham')\n",
    "hardham_path = os.path.join(DATASETS_PATH, 'hard_ham')\n",
    "spam_filenames = [filename for filename in os.listdir(spam_path) if len(filename) > 10]\n",
    "ham_filenames = [filename for filename in os.listdir(ham_path) if len(filename) > 10]\n",
    "hardham_filenames = [filename for filename in os.listdir(hardham_path) if len(filename) > 10]\n",
    "\n",
    "print(\"Spam emails:\", len(spam_filenames), \"\\nNon-spam emails:\", len(ham_filenames) + len(hardham_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_email(filename, directory):\n",
    "    with open(os.path.join(DATASETS_PATH, directory, filename), \"rb\") as f:        \n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "\n",
    "    \n",
    "spam_emails = [read_email(filename=filename, directory=\"spam\") for filename in spam_filenames]\n",
    "ham_emails = [read_email(filename=filename, directory=\"easy_ham\") for filename in ham_filenames]\n",
    "hardham_emails = [read_email(filename=filename, directory=\"hard_ham\") for filename in hardham_filenames]\n",
    "\n",
    "# unite ham and hardham into one nonspam category\n",
    "ham_emails = ham_emails + hardham_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n",
      "<HTML><HEAD>\n",
      "<META content=\"text/html; charset=windows-1252\" http-equiv=Content-Type>\n",
      "<META content=\"MSHTML 5.00.2314.1000\" name=GENERATOR></HEAD>\n",
      "<BODY><!-- Inserted by Calypso -->\n",
      "<TABLE border=0 cellPadding=0 cellSpacing=2 id=_CalyPrintHeader_ rules=none \n",
      "style=\"COLOR: black; DISPLAY: none\" width=\"100%\">\n",
      "  <TBODY>\n",
      "  <TR>\n",
      "    <TD colSpan=3>\n",
      "      <HR color=black noShade SIZE=1>\n",
      "    </TD></TR></TD></TR>\n",
      "  <TR>\n",
      "    <TD colSpan=3>\n",
      "      <HR color=black noShade SIZE=1>\n",
      "    </TD></TR></TBODY></TABLE><!-- End Calypso --><!-- Inserted by Calypso --><FONT \n",
      "color=#000000 face=VERDANA,ARIAL,HELVETICA size=-2><BR></FONT></TD></TR></TABLE><!-- End Calypso --><FONT color=#ff0000 \n",
      "face=\"Copperplate Gothic Bold\" size=5 PTSIZE=\"10\">\n",
      "<CENTER>Save up to 70% on Life Insurance.</CENTER></FONT><FONT color=#ff0000 \n",
      "face=\"Copperplate Gothic Bold\" size=5 PTSIZE=\"10\">\n",
      "<CENTER>Why Spend More Than You Have To?\n",
      "<CENTER><FONT color=#ff0000 face=\"Copperplate Gothic Bold\" size=5 PTSIZE=\"10\">\n",
      "<CENTER>Life Quote Savings\n",
      "<CENTER>\n",
      "<P align=left></P>\n",
      "<P align=left></P></FONT></U></I></B><BR></FONT></U></B></U></I>\n",
      "<P></P>\n",
      "<CENTER>\n",
      "<TABLE border=0 borderColor=#111111 cellPadding=0 cellSpacing=0 width=650>\n",
      "  <TBODY></TBODY></TABLE>\n",
      "<TABLE border=0 borderColor=#111111 cellPadding=5 cellSpacing=0 width=650>\n",
      "  <TBODY>\n",
      "  <TR>\n",
      "    <TD colSpan=2 width=\"35%\"><B><FONT face=Verdana size=4>Ensuring your \n",
      "      family's financial security is very important. Life Quote Savings makes \n",
      "      buying life insurance simple and affordable. We Provide FREE Access to The \n",
      "      Very Best Companies and The Lowest Rates.</FONT></B></TD></TR>\n",
      "  <TR>\n",
      "    <TD align=middle vAlign=top width=\"18%\">\n",
      "      <TABLE borderColor=#111111 width=\"100%\">\n",
      "        <TBODY>\n",
      "        <TR>\n",
      "          <TD style=\"PADDING-LEFT: 5px; PADDING-RIGHT: 5px\" width=\"100%\"><FONT \n",
      "            face=Verdana size=4><B>Life Quote Savings</B> is FAST, EASY and \n",
      "            SAVES you money! Let us help you get started with the best values in \n",
      "            the country on new coverage. You can SAVE hundreds or even thousands \n",
      "            of dollars by requesting a FREE quote from Lifequote Savings. Our \n",
      "            service will take you less than 5 minutes to complete. Shop and \n",
      "            compare. SAVE up to 70% on all types of Life insurance! \n",
      "</FONT></TD></TR>\n",
      "        <TR><BR><BR>\n",
      "          <TD height=50 style=\"PADDING-LEFT: 5px; PADDING-RIGHT: 5px\" \n",
      "          width=\"100%\">\n",
      "            <P align=center><B><FONT face=Verdana size=5><A \n",
      "            href=\"http://website.e365.cc/savequote/\">Click Here For Your \n",
      "            Free Quote!</A></FONT></B></P></TD>\n",
      "          <P><FONT face=Verdana size=4><STRONG>\n",
      "          <CENTER>Protecting your family is the best investment you'll ever \n",
      "          make!<BR></B></TD></TR>\n",
      "        <TR><BR><BR></STRONG></FONT></TD></TR></TD></TR>\n",
      "        <TR></TR></TBODY></TABLE>\n",
      "      <P align=left><FONT face=\"Arial, Helvetica, sans-serif\" size=2></FONT></P>\n",
      "      <P></P>\n",
      "      <CENTER><BR><BR><BR>\n",
      "      <P></P>\n",
      "      <P align=left><BR></B><BR><BR><BR><BR></P>\n",
      "      <P align=center><BR></P>\n",
      "      <P align=left><BR></B><BR><BR></FONT>If you are in receipt of this email \n",
      "      in error and/or wish to be removed from our list, <A \n",
      "      href=\"mailto:coins@btamail.net.cn\">PLEASE CLICK HERE</A> AND TYPE REMOVE. If you \n",
      "      reside in any state which prohibits e-mail solicitations for insurance, \n",
      "      please disregard this \n",
      "      email.<BR></FONT><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR></FONT></P></CENTER></CENTER></TR></TBODY></TABLE></CENTER></CENTER></CENTER></CENTER></CENTER></BODY></HTML>\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spam examle\n",
    "print(spam_emails[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Scotsman - 22 August 2002\n",
      "\n",
      " Playboy wants to go out with a bang \n",
      " \n",
      " \n",
      " AN AGEING Berlin playboy has come up with an unusual offer to lure women into\n",
      " his bed - by promising the last woman he sleeps with an inheritance of 250,000\n",
      " (Â£160,000). \n",
      " \n",
      " Rolf Eden, 72, a Berlin disco owner famous for his countless sex partners,\n",
      " said he could imagine no better way to die than in the arms of an attractive\n",
      " young woman - preferably under 30. \n",
      " \n",
      " \"I put it all in my last will and testament - the last woman who sleeps with\n",
      " me gets all the money,\" Mr Eden told Bild newspaper. \n",
      " \n",
      " \"I want to pass away in the most beautiful moment of my life. First a lot of\n",
      " fun with a beautiful woman, then wild sex, a final orgasm - and it will all\n",
      " end with a heart attack and then IÂm gone.\" \n",
      " \n",
      " Mr Eden, who is selling his nightclub this year, said applications should be\n",
      " sent in quickly because of his age. \"It could end very soon,\" he said.\n",
      "\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/ \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nonspam example\n",
    "print(ham_emails[6].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ham_emails + spam_emails\n",
    "y = [0]*len(ham_emails) + [1]*len(spam_emails)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting emails into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(raw_html):\n",
    "    cleantext = re.sub(r'<a.+?>', ' href url ', raw_html, flags=re.I | re.S | re.M) \n",
    "    cleantext = re.sub(r'<.+?>', ' ', cleantext, flags=re.S | re.M)\n",
    "    cleantext = re.sub(r'\\s+', ' ', cleantext) \n",
    "    return unescape(cleantext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Save up to 70% on Life Insurance. Why Spend More Than You Have To? Life Quote Savings Ensuring your family's financial security is very important. Life Quote Savings makes buying life insurance simple and affordable. We Provide FREE Access to The Very Best Companies and The Lowest Rates. Life Quote Savings is FAST, EASY and SAVES you money! Let us help you get started with the best values in the country on new coverage. You can SAVE hundreds or even thousands of dollars by requesting a FREE quote from Lifequote Savings. Our service will take you less than 5 minutes to complete. Shop and compare. SAVE up to 70% on all types of Life insurance! href url Click Here For Your Free Quote! Protecting your family is the best investment you'll ever make! If you are in receipt of this email in error and/or wish to be removed from our list, href url PLEASE CLICK HERE AND TYPE REMOVE. If you reside in any state which prohibits e-mail solicitations for insurance, please disregard this email. \n"
     ]
    }
   ],
   "source": [
    "cleaned_sample = clean_html(spam_emails[0].get_content())\n",
    "print(cleaned_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(raw_email):\n",
    "    html = None\n",
    "    for part in raw_email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if ctype not in ('text/html', 'text/plain'):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except:\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == 'text/plain':\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return clean_html(html)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Save up to 70% on Life Insurance. Why Spend More Than You Have To? Life Quote Savings Ensuring your ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(spam_emails[0])[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words in email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universe => univers\n",
      "Universal => univers\n",
      "University => univers\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "for word in (\"Universe\", \"Universal\", \"University\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "    print(word, \"=>\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universe, Universal, and University have the same stem but different meanings, however that wont be a huge problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.adclick.ws/p.cfm?o=315&s=pk007', 'http://www.adclick.ws/p.cfm?o=249&s=pk007', 'http://www.adclick.ws/p.cfm?o=245&s=pk002', 'http://www.adclick.ws/p.cfm?o=259&s=pk007', 'http://www.adclick.ws/p.cfm?o=283&s=pk007', 'http://www.qves.com/trim/?ilug@linux.ie%7C17%7C114258', 'http://www.linux.ie/mailman/listinfo/ilug']\n"
     ]
    }
   ],
   "source": [
    "url_extractor = urlextract.URLExtract()\n",
    "print(url_extractor.find_urls(email_to_text(spam_emails[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):    \n",
    "    def __init__(self, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "        self.replace_urls = replace_urls\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        if not isinstance(X, list):\n",
    "            X = [X]\n",
    "        for email in X:            \n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls:\n",
    "                urls = url_extractor.find_urls(text)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', text, flags=re.M | re.S)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M | re.S)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'save': 8, 'you': 8, 'to': 6, 'life': 6, 'and': 6, 'quot': 5, 'the': 5, 'insur': 4, 'in': 4, 'number': 3, 'on': 3, 'your': 3, 'is': 3, 'free': 3, 'best': 3, 'of': 3, 'up': 2, 'than': 2, 'famili': 2, 'veri': 2, 'make': 2, 'or': 2, 'from': 2, 'our': 2, 'type': 2, 'href': 2, 'url': 2, 'click': 2, 'here': 2, 'for': 2, 'if': 2, 'thi': 2, 'email': 2, 'remov': 2, 'pleas': 2, 'whi': 1, 'spend': 1, 'more': 1, 'have': 1, 'ensur': 1, 's': 1, 'financi': 1, 'secur': 1, 'import': 1, 'buy': 1, 'simpl': 1, 'afford': 1, 'we': 1, 'provid': 1, 'access': 1, 'compani': 1, 'lowest': 1, 'rate': 1, 'fast': 1, 'easi': 1, 'money': 1, 'let': 1, 'us': 1, 'help': 1, 'get': 1, 'start': 1, 'with': 1, 'valu': 1, 'countri': 1, 'new': 1, 'coverag': 1, 'can': 1, 'hundr': 1, 'even': 1, 'thousand': 1, 'dollar': 1, 'by': 1, 'request': 1, 'a': 1, 'lifequot': 1, 'servic': 1, 'will': 1, 'take': 1, 'less': 1, 'minut': 1, 'complet': 1, 'shop': 1, 'compar': 1, 'all': 1, 'protect': 1, 'invest': 1, 'll': 1, 'ever': 1, 'are': 1, 'receipt': 1, 'error': 1, 'wish': 1, 'be': 1, 'list': 1, 'resid': 1, 'ani': 1, 'state': 1, 'which': 1, 'prohibit': 1, 'e': 1, 'mail': 1, 'solicit': 1, 'disregard': 1}),\n",
       "       Counter({'number': 8, 'url': 7, 'the': 4, 'linux': 3, 'to': 2, 'day': 2, 'you': 2, 'list': 2, 'ie': 2, 'fight': 1, 'risk': 1, 'of': 1, 'cancer': 1, 'slim': 1, 'down': 1, 'guarante': 1, 'lose': 1, 'lb': 1, 'in': 1, 'get': 1, 'child': 1, 'support': 1, 'deserv': 1, 'free': 1, 'legal': 1, 'advic': 1, 'join': 1, 'web': 1, 's': 1, 'fastest': 1, 'grow': 1, 'singl': 1, 'commun': 1, 'start': 1, 'your': 1, 'privat': 1, 'photo': 1, 'album': 1, 'onlin': 1, 'have': 1, 'a': 1, 'wonder': 1, 'offer': 1, 'manag': 1, 'prizemama': 1, 'if': 1, 'wish': 1, 'leav': 1, 'thi': 1, 'pleas': 1, 'use': 1, 'link': 1, 'below': 1, 'irish': 1, 'user': 1, 'group': 1, 'ilug': 1, 'for': 1, 'un': 1, 'subscript': 1, 'inform': 1, 'maintain': 1, 'listmast': 1}),\n",
       "       Counter({'number': 8, 'url': 6, 'the': 4, 'to': 2, 'day': 2, 'you': 2, 'fight': 1, 'risk': 1, 'of': 1, 'cancer': 1, 'slim': 1, 'down': 1, 'guarante': 1, 'lose': 1, 'lb': 1, 'in': 1, 'get': 1, 'child': 1, 'support': 1, 'deserv': 1, 'free': 1, 'legal': 1, 'advic': 1, 'join': 1, 'web': 1, 's': 1, 'fastest': 1, 'grow': 1, 'singl': 1, 'commun': 1, 'start': 1, 'your': 1, 'privat': 1, 'photo': 1, 'album': 1, 'onlin': 1, 'have': 1, 'a': 1, 'wonder': 1, 'offer': 1, 'manag': 1, 'prizemama': 1, 'if': 1, 'wish': 1, 'leav': 1, 'thi': 1, 'list': 1, 'pleas': 1, 'use': 1, 'link': 1, 'below': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_email = spam_emails[:3]\n",
    "some_email_wordcounts = EmailToWordCounterTransformer().fit_transform(some_email)\n",
    "some_email_wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "Now it's time to vectorize all counted words:\n",
    " \n",
    "1. First I will count all words\n",
    "2. Most common of them I will store in a N=1000 size dictionary \n",
    "3. All my vectors I will store in a <a href=\"https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\">sparce matrix</a> to decrease computational and space costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):    \n",
    "    def __init__(self, vocabulary_size=1000, word_max_occurrence=10):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.word_max_occurrence = word_max_occurrence\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()  \n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, self.word_max_occurrence)  # to prevent word occurrence more than X times at one email\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}  # +1 for top-1 (not top-0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for i, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(i)                \n",
    "                cols.append(self.vocabulary_.get(word, 0))  # Store a word's index. \n",
    "                                                            # If a word isn't common then append it to index 0\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 25 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "few_vectors = vocab_transformer.fit_transform(some_email_wordcounts)\n",
    "few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 1,\n",
       " 'url': 2,\n",
       " 'the': 3,\n",
       " 'you': 4,\n",
       " 'to': 5,\n",
       " 'save': 6,\n",
       " 'life': 7,\n",
       " 'and': 8,\n",
       " 'in': 9,\n",
       " 'quot': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top N=10 of most common words\n",
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126,   3,   2,   5,   8,   6,   8,   6,   6,   4,   5],\n",
       "       [ 62,   8,   7,   4,   2,   2,   0,   0,   0,   1,   0],\n",
       "       [ 46,   8,   6,   4,   2,   2,   0,   0,   0,   1,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array shows that the first email has 126 non common words, 3 'number', 2 'url', ... , 4 'in', 5 'quot'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = Pipeline([\n",
    "    ('email_to_words', EmailToWordCounterTransformer()),\n",
    "    ('words_to_vectors', WordCounterToVectorTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use multiple scorers to evaluate models\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "def print_scores(score):\n",
    "    #print(\"Accuracy:  {:0.2f}%\".format(100*score['test_accuracy'].mean()))\n",
    "    print(\"Precision: {:0.2f}%\".format(100*score['test_precision'].mean()))\n",
    "    print(\"Recall:    {:0.2f}%\".format(100*score['test_recall'].mean()))\n",
    "    print(\"F1 score:  {:0.2f}%\".format(100*score['test_f1'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.971, f1=0.905, precision=0.930, recall=0.881, total=   0.2s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  , accuracy=0.967, f1=0.888, precision=0.927, recall=0.852, total=   0.3s\n",
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.973, f1=0.909, precision=0.966, recall=0.858, total=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "log_clsf = LogisticRegression(solver='liblinear', random_state=11)\n",
    "score = cross_validate(log_clsf, X_train_prepared, y_train, cv=3, verbose=3, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 94.12%\n",
      "Recall:    86.38%\n",
      "F1 score:  90.07%\n"
     ]
    }
   ],
   "source": [
    "print_scores(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that I need to increase max_iter to achieve convergence and better results. But I leave it to a grid search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.940, f1=0.770, precision=0.956, recall=0.644, total=   2.0s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  , accuracy=0.943, f1=0.786, precision=0.957, recall=0.667, total=   1.9s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  , accuracy=0.942, f1=0.775, precision=0.977, recall=0.642, total=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "sv_clsf = SVC(gamma='auto', random_state=11)\n",
    "score = cross_validate(sv_clsf, X_train_prepared, y_train, cv=3, verbose=3, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 96.36%\n",
      "Recall:    65.10%\n",
      "F1 score:  77.69%\n"
     ]
    }
   ],
   "source": [
    "print_scores(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  , accuracy=0.913, f1=0.641, precision=0.905, recall=0.496, total=   0.6s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  , accuracy=0.915, f1=0.644, precision=0.918, recall=0.496, total=   0.6s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  , accuracy=0.924, f1=0.705, precision=0.878, recall=0.590, total=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nk_clsf = KNeighborsClassifier()\n",
    "score = cross_validate(nk_clsf, X_train_prepared, y_train, cv=3, verbose=3, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 90.03%\n",
      "Recall:    52.74%\n",
      "F1 score:  66.36%\n"
     ]
    }
   ],
   "source": [
    "print_scores(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.939, f1=0.801, precision=0.811, recall=0.793, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.934, f1=0.799, precision=0.764, recall=0.837, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.938, f1=0.791, precision=0.823, recall=0.761, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clsf = DecisionTreeClassifier()\n",
    "score = cross_validate(tree_clsf, X_train_prepared, y_train, cv=3, verbose=3, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 79.89%\n",
      "Recall:    79.69%\n",
      "F1 score:  79.69%\n"
     ]
    }
   ],
   "source": [
    "print_scores(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.935, f1=0.765, precision=0.883, recall=0.674, total=   8.8s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  , accuracy=0.920, f1=0.732, precision=0.770, recall=0.696, total=   8.8s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   17.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  , accuracy=0.927, f1=0.736, precision=0.838, recall=0.657, total=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   26.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "gaus_clsf = GaussianProcessClassifier()\n",
    "score = cross_validate(gaus_clsf, X_train_prepared.toarray(), y_train, cv=3, verbose=3, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 83.07%\n",
      "Recall:    67.57%\n",
      "F1 score:  74.42%\n"
     ]
    }
   ],
   "source": [
    "print_scores(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision/Recall tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two best models: \n",
    "- with the highest precision (SVC), \n",
    "- and with the highest recall (Logistic regression). \n",
    "\n",
    "I guess that precision is more important than recall, because you don't want nonspam email to be marked as spam. But logistic regression shows very good results so let's tune both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipeline = Pipeline([\n",
    "    ('preprocess', preprocess_pipeline),\n",
    "    ('model', LogisticRegression(solver='liblinear', random_state=11))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {#'preprocess__email_to_words__lower_case': [True, False], \n",
    "    #'preprocess__email_to_words__remove_punctuation': [True, False],\n",
    "    #'preprocess__email_to_words__replace_urls': [True, False],\n",
    "    #'preprocess__email_to_words__replace_numbers': [True, False],\n",
    "    #'preprocess__email_to_words__stemming': [True, False],\n",
    "    #'preprocess__words_to_vectors__vocabulary_size': [500, 1000, 1500],\n",
    "    'preprocess__words_to_vectors__word_max_occurrence': [10, 15, 20],\n",
    "    'model__max_iter': [300, 1000]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(log_pipeline, param_grid=param_grid, cv=3, scoring='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocess',\n",
       "                                        Pipeline(memory=None,\n",
       "                                                 steps=[('email_to_words',\n",
       "                                                         EmailToWordCounterTransformer(lower_case=True,\n",
       "                                                                                       remove_punctuation=True,\n",
       "                                                                                       replace_numbers=True,\n",
       "                                                                                       replace_urls=True,\n",
       "                                                                                       stemming=True)),\n",
       "                                                        ('words_to_vectors',\n",
       "                                                         WordCounterToVectorTransformer(vocabulary_size=1000,\n",
       "                                                                                        word_...\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=11,\n",
       "                                                           solver='liblinear',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'model__max_iter': [300, 1000],\n",
       "                          'preprocess__words_to_vectors__word_max_occurrence': [10,\n",
       "                                                                                15,\n",
       "                                                                                20]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='precision', verbose=0)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__max_iter': 300,\n",
       " 'preprocess__words_to_vectors__word_max_occurrence': 10}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([10.85013223, 10.74912071, 10.81650233, 10.85602458, 10.74093119,\n",
       "        10.80008578]),\n",
       " 'std_fit_time': array([0.42345079, 0.43909707, 0.51647215, 0.46482418, 0.33533684,\n",
       "        0.49776509]),\n",
       " 'mean_score_time': array([5.06636898, 5.00877579, 5.00778302, 4.99763799, 5.01524131,\n",
       "        5.02086687]),\n",
       " 'std_score_time': array([0.41331218, 0.45380918, 0.44979927, 0.4516597 , 0.45071123,\n",
       "        0.46328714]),\n",
       " 'param_model__max_iter': masked_array(data=[300, 300, 300, 1000, 1000, 1000],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocess__words_to_vectors__word_max_occurrence': masked_array(data=[10, 15, 20, 10, 15, 20],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20}],\n",
       " 'split0_test_score': array([0.92248062, 0.92248062, 0.91538462, 0.92248062, 0.92248062,\n",
       "        0.91538462]),\n",
       " 'split1_test_score': array([0.89473684, 0.89473684, 0.89473684, 0.89473684, 0.89473684,\n",
       "        0.89473684]),\n",
       " 'split2_test_score': array([0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375]),\n",
       " 'mean_test_score': array([0.92603824, 0.92603824, 0.92367199, 0.92603824, 0.92603824,\n",
       "        0.92367199]),\n",
       " 'std_test_score': array([0.02714061, 0.02714061, 0.02765214, 0.02714061, 0.02714061,\n",
       "        0.02765214]),\n",
       " 'rank_test_score': array([1, 1, 5, 1, 1, 5], dtype=int32)}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  98.31%\n",
      "Precision: 97.75%\n",
      "Recall:    90.62%\n",
      "F1 score:  94.05%\n"
     ]
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test_prepared = final_model.named_steps['preprocess'].transform(X_test)\n",
    "y_test_predicted = final_model.named_steps['model'].predict(X_test_prepared)\n",
    "\n",
    "print(\"Accuracy:  {:.2f}%\".format(100*accuracy_score(y_test, y_test_predicted)))\n",
    "print(\"Precision: {:.2f}%\".format(100*precision_score(y_test, y_test_predicted)))\n",
    "print(\"Recall:    {:.2f}%\".format(100*recall_score(y_test, y_test_predicted)))\n",
    "print(\"F1 score:  {:.2f}%\".format(100*f1_score(y_test, y_test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipeline = Pipeline([\n",
    "    ('preprocess', preprocess_pipeline),\n",
    "    ('model', SVC(random_state=11))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'preprocess__words_to_vectors__word_max_occurrence': [10, 15, 20],\n",
    "     'model__max_iter': [300, 1000],\n",
    "     'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "     'model__gamma': ['auto', 'scale']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_svc_search = GridSearchCV(svc_pipeline, param_grid=param_grid, cv=3, scoring='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\batyr\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocess',\n",
       "                                        Pipeline(memory=None,\n",
       "                                                 steps=[('email_to_words',\n",
       "                                                         EmailToWordCounterTransformer(lower_case=True,\n",
       "                                                                                       remove_punctuation=True,\n",
       "                                                                                       replace_numbers=True,\n",
       "                                                                                       replace_urls=True,\n",
       "                                                                                       stemming=True)),\n",
       "                                                        ('words_to_vectors',\n",
       "                                                         WordCounterToVectorTransformer(vocabulary_size=1000,\n",
       "                                                                                        word_...\n",
       "                                            shrinking=True, tol=0.001,\n",
       "                                            verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'model__gamma': ['auto', 'scale'],\n",
       "                          'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
       "                          'model__max_iter': [300, 1000],\n",
       "                          'preprocess__words_to_vectors__word_max_occurrence': [10,\n",
       "                                                                                15,\n",
       "                                                                                20]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='precision', verbose=0)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__gamma': 'auto',\n",
       " 'model__kernel': 'rbf',\n",
       " 'model__max_iter': 300,\n",
       " 'preprocess__words_to_vectors__word_max_occurrence': 15}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([10.32315397, 10.31162437, 10.33002385, 10.42916759, 10.40327819,\n",
       "        10.56731574, 10.37884514, 10.37514544, 10.43473983, 10.44453049,\n",
       "        10.51623551, 10.45016217, 10.72533878, 10.71461527, 10.75441138,\n",
       "        10.93969878, 11.05086263, 11.10578275, 11.03459605, 10.75838669,\n",
       "        10.77371955, 10.76736196, 10.73797035, 10.82863164, 10.39912804,\n",
       "        10.45134219, 10.37079716, 10.41054893, 10.51512019, 10.47347879,\n",
       "        10.74345954, 10.6723338 , 10.6646359 , 10.79562823, 10.73335004,\n",
       "        10.75748324, 10.75226974, 10.73927649, 10.73590573, 10.86317913,\n",
       "        10.80771724, 10.82409358, 10.74699457, 10.79978299, 11.11934034,\n",
       "        10.59423184, 10.46911669, 10.50475677]),\n",
       " 'std_fit_time': array([0.46333373, 0.46108606, 0.46694459, 0.42019165, 0.45678291,\n",
       "        0.44444297, 0.44690363, 0.44635366, 0.46322695, 0.49660734,\n",
       "        0.47710687, 0.48124274, 0.37353226, 0.42496444, 0.46365604,\n",
       "        0.46886263, 0.4439696 , 0.55556929, 0.3572767 , 0.41844966,\n",
       "        0.40075325, 0.43291659, 0.46799706, 0.39558813, 0.46975885,\n",
       "        0.49292076, 0.45984293, 0.44473245, 0.32492363, 0.50245902,\n",
       "        0.4945099 , 0.49299901, 0.50630048, 0.51675691, 0.47750025,\n",
       "        0.5207467 , 0.45287619, 0.4569741 , 0.45842507, 0.42110395,\n",
       "        0.47167475, 0.43951056, 0.4928147 , 0.46627148, 0.46774767,\n",
       "        0.41825199, 0.48634067, 0.50747132]),\n",
       " 'mean_score_time': array([5.06627425e+00, 5.06562789e+00, 5.08090035e+00, 5.08968608e+00,\n",
       "        5.09589553e+00, 5.14725828e+00, 5.10512463e+00, 5.09913834e+00,\n",
       "        5.10790332e+00, 5.09782648e+00, 5.12406683e+00, 5.11386347e+00,\n",
       "        5.28369069e+00, 5.27198283e+00, 5.29199855e+00, 5.38118140e+00,\n",
       "        5.47422282e+00, 5.43554107e+00, 5.33517917e+00, 5.30905652e+00,\n",
       "        5.29316417e+00, 5.28431392e+00, 5.29091303e+00, 5.33197792e+00,\n",
       "        5.10412161e+00, 5.09584697e+00, 5.09715263e+00, 5.11759845e+00,\n",
       "        5.19538180e+00, 5.09130812e+00, 5.27045425e+00, 5.24181549e+00,\n",
       "        5.23339971e+00, 5.31989169e+00, 5.24605290e+00, 5.27274680e+00,\n",
       "        5.26018516e+00, 5.27547963e+00, 5.25745559e+00, 5.28799264e+00,\n",
       "        5.29002945e+00, 5.29180463e+00, 5.27268998e+00, 5.26089144e+00,\n",
       "        1.30384034e+04, 5.19868231e+00, 5.11208534e+00, 5.13297653e+00]),\n",
       " 'std_score_time': array([4.28656356e-01, 4.37665465e-01, 4.36334819e-01, 4.41970163e-01,\n",
       "        4.33961076e-01, 4.66738695e-01, 4.48165414e-01, 4.37406407e-01,\n",
       "        4.33450364e-01, 4.24416332e-01, 4.39206319e-01, 4.57905669e-01,\n",
       "        4.89309573e-01, 4.50910868e-01, 4.39121388e-01, 4.35653877e-01,\n",
       "        3.56115776e-01, 4.44237952e-01, 4.72656622e-01, 4.92727822e-01,\n",
       "        4.44439980e-01, 4.37893186e-01, 4.28255039e-01, 4.16354822e-01,\n",
       "        4.27544286e-01, 4.43110383e-01, 4.32579557e-01, 4.27698474e-01,\n",
       "        5.06248480e-01, 4.28836995e-01, 4.61931133e-01, 4.12861294e-01,\n",
       "        4.51620466e-01, 5.03068253e-01, 4.37180054e-01, 4.23834054e-01,\n",
       "        4.39757382e-01, 4.44565848e-01, 4.31527256e-01, 4.17947506e-01,\n",
       "        4.89798603e-01, 4.63121962e-01, 4.18237411e-01, 4.16197851e-01,\n",
       "        1.84321952e+04, 4.09153779e-01, 4.12301980e-01, 4.20895919e-01]),\n",
       " 'param_model__gamma': masked_array(data=['auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto',\n",
       "                    'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto',\n",
       "                    'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto',\n",
       "                    'auto', 'auto', 'auto', 'scale', 'scale', 'scale',\n",
       "                    'scale', 'scale', 'scale', 'scale', 'scale', 'scale',\n",
       "                    'scale', 'scale', 'scale', 'scale', 'scale', 'scale',\n",
       "                    'scale', 'scale', 'scale', 'scale', 'scale', 'scale',\n",
       "                    'scale', 'scale', 'scale'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__kernel': masked_array(data=['linear', 'linear', 'linear', 'linear', 'linear',\n",
       "                    'linear', 'poly', 'poly', 'poly', 'poly', 'poly',\n",
       "                    'poly', 'rbf', 'rbf', 'rbf', 'rbf', 'rbf', 'rbf',\n",
       "                    'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid',\n",
       "                    'sigmoid', 'linear', 'linear', 'linear', 'linear',\n",
       "                    'linear', 'linear', 'poly', 'poly', 'poly', 'poly',\n",
       "                    'poly', 'poly', 'rbf', 'rbf', 'rbf', 'rbf', 'rbf',\n",
       "                    'rbf', 'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid',\n",
       "                    'sigmoid', 'sigmoid'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__max_iter': masked_array(data=[300, 300, 300, 1000, 1000, 1000, 300, 300, 300, 1000,\n",
       "                    1000, 1000, 300, 300, 300, 1000, 1000, 1000, 300, 300,\n",
       "                    300, 1000, 1000, 1000, 300, 300, 300, 1000, 1000, 1000,\n",
       "                    300, 300, 300, 1000, 1000, 1000, 300, 300, 300, 1000,\n",
       "                    1000, 1000, 300, 300, 300, 1000, 1000, 1000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocess__words_to_vectors__word_max_occurrence': masked_array(data=[10, 15, 20, 10, 15, 20, 10, 15, 20, 10, 15, 20, 10, 15,\n",
       "                    20, 10, 15, 20, 10, 15, 20, 10, 15, 20, 10, 15, 20, 10,\n",
       "                    15, 20, 10, 15, 20, 10, 15, 20, 10, 15, 20, 10, 15, 20,\n",
       "                    10, 15, 20, 10, 15, 20],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__gamma': 'auto',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'auto',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'linear',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'poly',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'rbf',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 300,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 10},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 15},\n",
       "  {'model__gamma': 'scale',\n",
       "   'model__kernel': 'sigmoid',\n",
       "   'model__max_iter': 1000,\n",
       "   'preprocess__words_to_vectors__word_max_occurrence': 20}],\n",
       " 'split0_test_score': array([0.78321678, 0.74404762, 0.19083969, 0.7704918 , 0.56363636,\n",
       "        0.81481481, 0.17437252, 0.15625   , 0.15527231, 0.15985577,\n",
       "        0.        , 0.31578947, 0.94845361, 0.95698925, 0.95698925,\n",
       "        0.95604396, 0.95604396, 0.95555556, 0.24509804, 0.25252525,\n",
       "        0.25      , 0.24509804, 0.25252525, 0.25      , 0.78321678,\n",
       "        0.74404762, 0.19083969, 0.7704918 , 0.56363636, 0.81481481,\n",
       "        0.15570934, 0.75      , 0.66666667, 0.66666667, 0.66666667,\n",
       "        0.15588915, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.33333333, 0.3       , 0.3       ,\n",
       "        0.27272727, 0.33333333, 0.33333333]),\n",
       " 'split1_test_score': array([0.8317757 , 0.36309524, 0.87096774, 0.36666667, 0.88      ,\n",
       "        0.17862372, 0.15563298, 0.6       , 0.15606936, 0.80952381,\n",
       "        0.15661253, 0.66666667, 0.94059406, 0.95833333, 0.94791667,\n",
       "        0.95744681, 0.95744681, 0.95744681, 0.41463415, 0.4       ,\n",
       "        0.41666667, 0.41463415, 0.4       , 0.41666667, 0.8317757 ,\n",
       "        0.36309524, 0.87096774, 0.36666667, 0.88      , 0.17862372,\n",
       "        0.5       , 0.15491329, 0.15606936, 0.15661253, 0.15606936,\n",
       "        0.1618705 , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.08411215, 0.08571429, 0.08411215,\n",
       "        0.08411215, 0.08571429, 0.08411215]),\n",
       " 'split2_test_score': array([0.68648649, 0.2519084 , 0.23445826, 0.89051095, 0.7804878 ,\n",
       "        0.79439252, 0.15411356, 0.15890084, 0.15527231, 0.15491329,\n",
       "        0.15384615, 0.15473441, 0.97727273, 0.97727273, 0.97727273,\n",
       "        0.97727273, 0.97727273, 0.97727273, 0.31531532, 0.29464286,\n",
       "        0.3       , 0.31531532, 0.29464286, 0.3       , 0.68648649,\n",
       "        0.2519084 , 0.23445826, 0.89051095, 0.7804878 , 0.79439252,\n",
       "        0.        , 0.15473441, 1.        , 0.15491329, 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.3       , 0.3       , 0.22222222,\n",
       "        0.3       , 0.3       , 0.22222222]),\n",
       " 'mean_test_score': array([0.76719069, 0.45309443, 0.43216458, 0.67580726, 0.74135968,\n",
       "        0.59586736, 0.16137581, 0.30510649, 0.15553809, 0.37484885,\n",
       "        0.10346686, 0.3791498 , 0.95543173, 0.96419341, 0.96071985,\n",
       "        0.96358257, 0.96358257, 0.9634197 , 0.32501956, 0.31573081,\n",
       "        0.32223077, 0.32501956, 0.31573081, 0.32223077, 0.76719069,\n",
       "        0.45309443, 0.43216458, 0.67580726, 0.74135968, 0.59586736,\n",
       "        0.21865385, 0.35329224, 0.60742775, 0.32612999, 0.60742775,\n",
       "        0.43903754, 0.66692308, 0.66692308, 0.66692308, 0.66692308,\n",
       "        0.66692308, 0.66692308, 0.23912509, 0.22854396, 0.20210372,\n",
       "        0.2189153 , 0.23965934, 0.21321911]),\n",
       " 'std_test_score': array([6.03818978e-02, 2.10739944e-01, 3.10880520e-01, 2.24079962e-01,\n",
       "        1.32106130e-01, 2.95238640e-01, 9.21361289e-03, 2.08584161e-01,\n",
       "        3.75772976e-04, 3.07456898e-01, 7.31919347e-02, 2.13726730e-01,\n",
       "        1.57651120e-02, 9.25942002e-03, 1.22704833e-02, 9.69176358e-03,\n",
       "        9.69176358e-03, 9.82032675e-03, 6.95650031e-02, 6.20345773e-02,\n",
       "        6.98443183e-02, 6.95650031e-02, 6.20345773e-02, 6.98443183e-02,\n",
       "        6.03818978e-02, 2.10739944e-01, 3.10880520e-01, 2.24079962e-01,\n",
       "        1.32106130e-01, 2.95238640e-01, 2.08903778e-01, 2.80595678e-01,\n",
       "        3.47038792e-01, 2.40866255e-01, 3.47038792e-01, 3.96439059e-01,\n",
       "        4.71313788e-01, 4.71313788e-01, 4.71313788e-01, 4.71313788e-01,\n",
       "        4.71313788e-01, 4.71313788e-01, 1.10483429e-01, 1.01024963e-01,\n",
       "        8.92919850e-02, 9.59954718e-02, 1.09733892e-01, 1.01962391e-01]),\n",
       " 'rank_test_score': array([ 7, 23, 26, 11,  9, 21, 46, 38, 47, 29, 48, 28,  6,  1,  5,  2,  2,\n",
       "         4, 32, 36, 34, 32, 36, 34,  7, 23, 26, 11,  9, 21, 43, 30, 19, 31,\n",
       "        19, 25, 13, 13, 13, 13, 13, 13, 40, 41, 45, 42, 39, 44],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  96.46%\n",
      "Precision: 96.20%\n",
      "Recall:    79.17%\n",
      "F1 score:  86.86%\n"
     ]
    }
   ],
   "source": [
    "final_svc_model = grid_svc_search.best_estimator_\n",
    "\n",
    "X_test_prepared = final_svc_model.named_steps['preprocess'].transform(X_test)\n",
    "y_test_predicted = final_svc_model.named_steps['model'].predict(X_test_prepared)\n",
    "\n",
    "print(\"Accuracy:  {:.2f}%\".format(100*accuracy_score(y_test, y_test_predicted)))\n",
    "print(\"Precision: {:.2f}%\".format(100*precision_score(y_test, y_test_predicted)))\n",
    "print(\"Recall:    {:.2f}%\".format(100*recall_score(y_test, y_test_predicted)))\n",
    "print(\"F1 score:  {:.2f}%\".format(100*f1_score(y_test, y_test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the end logistic regression has better score than SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing results\n",
    "\n",
    "Let's see most spammish and anti-spam words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = final_model.named_steps['model'].coef_[0]\n",
    "vocabulary = preprocess_pipeline.named_steps['words_to_vectors'].vocabulary_\n",
    "vocabulary['noncommon'] = 0\n",
    "spam_words = [(value, word) for value, word in list(zip(coefs, vocabulary)) if value > 0]\n",
    "ham_words = [(value, word) for value, word in list(zip(coefs, vocabulary)) if value < 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 anti-spam markers:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-1.9900630803994135, 'd'),\n",
       " (-1.3405669293752922, 'see'),\n",
       " (-0.9185355281695274, 'have'),\n",
       " (-0.8061406833120347, 'inform'),\n",
       " (-0.6388523098041495, 'spamassassin'),\n",
       " (-0.6196092975709157, 'org'),\n",
       " (-0.5942805122837375, 'current'),\n",
       " (-0.5756259209025084, 'which'),\n",
       " (-0.5700023784666036, 'go'),\n",
       " (-0.566600752737371, 'american'),\n",
       " (-0.5652089112564591, 'free'),\n",
       " (-0.5590167650843902, 'doe'),\n",
       " (-0.5430979819724608, 'date'),\n",
       " (-0.5296523971809944, 'subject'),\n",
       " (-0.5287122674188766, 'right'),\n",
       " (-0.526223560098543, 'on'),\n",
       " (-0.5240506859922857, 'aug'),\n",
       " (-0.5219964106632539, 'color'),\n",
       " (-0.5045922895958341, 'data'),\n",
       " (-0.4965543594207527, 'with')]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 20 anti-spam markers:\")\n",
    "ham_words.sort()\n",
    "ham_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 spam markers:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0961410848719069, 'help'),\n",
       " (1.0221423787914414, 'those'),\n",
       " (0.8513502918158704, 'complet'),\n",
       " (0.804486790147418, 'photo'),\n",
       " (0.7153494529549685, 'what'),\n",
       " (0.6830160983984985, 'time'),\n",
       " (0.6825047190330646, 'someon'),\n",
       " (0.67248135893572, 'isn'),\n",
       " (0.6662179002158571, 'consum'),\n",
       " (0.6428025988536068, 'includ'),\n",
       " (0.6417316768040378, 'ham'),\n",
       " (0.6325165958476017, 'world'),\n",
       " (0.6249335006459942, 'make'),\n",
       " (0.6209286761215584, 'into'),\n",
       " (0.6185447436926546, 'befor'),\n",
       " (0.588716315324631, 'two'),\n",
       " (0.5737462821396965, 'new'),\n",
       " (0.5700697446919565, 'link'),\n",
       " (0.5466557992716997, 'forc'),\n",
       " (0.5425950103757103, 'polit')]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 20 spam markers:\")\n",
    "spam_words.sort(reverse=True)\n",
    "spam_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
